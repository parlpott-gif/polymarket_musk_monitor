#!/usr/bin/env python3
"""
é©¬æ–¯å…‹æ¨æ–‡ç›‘æ§è„šæœ¬ v3 - æœ€ç»ˆç‰ˆ
æŠ“å–æ¨æ–‡ -> æ›´æ–° tweets.json -> æ›´æ–° HTML
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime
import os

# Config
PROJECT_DIR = "/home/admin/polymarket_musk_monitor"
OUTPUT_FILE = f"{PROJECT_DIR}/tweets.json"
HTML_FILE = f"{PROJECT_DIR}/index.html"

SCRAPE_SOURCES = [
    ("xcancel", "https://xcancel.com/elonmusk"),
]

LAST_TWEET_LINK = None

def get_tweets_from_scrape():
    """ä»ç½‘é¡µæŠ“å–æ¨æ–‡"""
    tweets = []
    
    for name, url in SCRAPE_SOURCES:
        try:
            print(f"æŠ“å–: {name}")
            resp = requests.get(url, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            if resp.status_code != 200:
                print(f"âœ— HTTP {resp.status_code}")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            tweet_divs = soup.find_all('div', class_='tweet-content')
            
            if tweet_divs:
                print(f"âœ“ è·å– {len(tweet_divs)} æ¡æ¨æ–‡")
                links = soup.find_all('a', class_='tweet-link')
                
                for i, div in enumerate(tweet_divs):
                    content = div.get_text().strip()
                    link = links[i].get('href', '') if i < len(links) else f"tweet_{i}"
                    
                    tweets.append({
                        'id': i + 1,
                        'content': content,
                        'link': f"https://xcancel.com{link}" if link.startswith('/') else link,
                        'username': 'Elon Musk',
                        'handle': '@elonmusk',
                        'source': name
                    })
                break
                    
        except Exception as e:
            print(f"âœ— å¤±è´¥: {str(e)[:50]}")
            continue
    
    return tweets

def update_html(tweets):
    """æ›´æ–° HTML æ–‡ä»¶ï¼ŒåµŒå…¥æ¨æ–‡æ•°æ®"""
    
    # ç”Ÿæˆæ¨æ–‡ HTML
    tweets_html = ""
    for tweet in tweets[:10]:  # åªæ˜¾ç¤ºå‰10æ¡
        # æˆªæ–­å¤ªé•¿çš„å†…å®¹
        content = tweet['content'][:200]
        if len(tweet['content']) > 200:
            content += "..."
            
        tweets_html += f'''
        <div class="tweet">
            <div class="tweet-header">
                <div class="avatar"></div>
                <div class="user-info">
                    <div class="username">{tweet['username']} <span class="badge">ğŸ¦</span></div>
                    <div class="handle">{tweet['handle']}</div>
                </div>
            </div>
            <div class="tweet-content">{content}</div>
            <div class="tweet-link"><a href="{tweet['link']}" target="_blank">æŸ¥çœ‹åŸæ–‡ â†’</a></div>
        </div>'''
    
    # è¯»å–æ¨¡æ¿
    with open(HTML_FILE, 'r', encoding='utf-8') as f:
        html = f.read()
    
    # æ›¿æ¢æ¨æ–‡å®¹å™¨å†…å®¹
    html = html.replace(
        '<!-- Tweets will be embedded here -->',
        tweets_html
    )
    
    # æ›´æ–°ç»Ÿè®¡
    html = html.replace(
        'id="tweetCount">0',
        f'id="tweetCount">{len(tweets)}'
    )
    html = html.replace(
        'id="lastUpdate">--',
        f'id="lastUpdate">{datetime.now().strftime("%H:%M")}'
    )
    
    # å†™å›
    with open(HTML_FILE, 'w', encoding='utf-8') as f:
        f.write(html)
    
    print(f"âœ“ HTML å·²æ›´æ–°")

def check_updates():
    """æ£€æŸ¥å¹¶æ›´æ–°"""
    global LAST_TWEET_LINK
    
    print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] === æ£€æŸ¥æ¨æ–‡ ===")
    
    tweets = get_tweets_from_scrape()
    
    if not tweets:
        print("æŠ“å–å¤±è´¥")
        return
    
    # æ£€æŸ¥æ–°æ¨æ–‡
    new_count = 0
    if LAST_TWEET_LINK:
        for tweet in tweets:
            if tweet['link'] != LAST_TWEET_LINK:
                new_count += 1
    else:
        new_count = len(tweets)
    
    if new_count > 0:
        print(f"ğŸ‰ {new_count} æ¡æ–°æ¨æ–‡!")
        LAST_TWEET_LINK = tweets[0]['link']
    else:
        print("æ— æ–°æ¨æ–‡")
    
    # ä¿å­˜ JSON
    data = {
        'last_updated': datetime.now().isoformat(),
        'count': len(tweets),
        'source': tweets[0]['source'] if tweets else 'none',
        'new_tweets': new_count,
        'tweets': tweets
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"âœ“ JSON å·²ä¿å­˜: {len(tweets)} æ¡")
    
    # æ›´æ–° HTML
    update_html(tweets)
    
    print("=== å®Œæˆ ===\n")

def main():
    global LAST_TWEET_LINK
    
    # è¯»å–ç¼“å­˜
    try:
        with open(OUTPUT_FILE, 'r') as f:
            old = json.load(f)
            if old.get('tweets'):
                LAST_TWEET_LINK = old['tweets'][0].get('link')
    except:
        pass
    
    check_updates()

if __name__ == "__main__":
    main()
