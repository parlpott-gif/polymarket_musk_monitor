#!/usr/bin/env python3
"""
é©¬æ–¯å…‹æ¨æ–‡ç›‘æ§è„šæœ¬ v3
ä½¿ç”¨ BeautifulSoup ä» xcancel.com æŠ“å–æ¨æ–‡
"""

import requests
from bs4 import BeautifulSoup
import json
from datetime import datetime

# Config
OUTPUT_FILE = "/home/admin/polymarket_musk_monitor/tweets.json"
CHECK_INTERVAL = 300  # 5 minutes

# æŠ“å–æº
SCRAPE_SOURCES = [
    ("xcancel", "https://xcancel.com/elonmusk"),
]

# è®°å½•ä¸Šæ¬¡æ¨æ–‡ ID
LAST_TWEET_LINK = None

def get_tweets_from_scrape():
    """ä»ç½‘é¡µæŠ“å–æ¨æ–‡"""
    tweets = []
    
    for name, url in SCRAPE_SOURCES:
        try:
            print(f"æŠ“å–: {name} - {url}")
            resp = requests.get(url, timeout=30, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
            })
            
            if resp.status_code != 200:
                print(f"âœ— HTTP {resp.status_code}")
                continue
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # æ‰¾æ¨æ–‡å®¹å™¨
            tweet_divs = soup.find_all('div', class_='tweet-content')
            
            if tweet_divs:
                print(f"âœ“ æˆåŠŸè·å– {len(tweet_divs)} æ¡æ¨æ–‡")
                
                # è·å–é“¾æ¥ï¼ˆç”¨äºå»é‡ï¼‰
                links = soup.find_all('a', class_='tweet-link')
                
                for i, div in enumerate(tweet_divs):
                    content = div.get_text().strip()
                    link = links[i].get('href', '') if i < len(links) else f"tweet_{i}"
                    
                    tweets.append({
                        'id': i + 1,
                        'content': content,
                        'link': link,
                        'username': 'Elon Musk',
                        'handle': '@elonmusk',
                        'source': name
                    })
                break
            else:
                # å°è¯•å…¶ä»– class
                tweet_divs = soup.find_all('a', class_='post')
                if tweet_divs:
                    print(f"âœ“ æ‰¾åˆ° {len(tweet_divs)} æ¡ (ç”¨ post class)")
                    for i, a in enumerate(tweet_divs[:10]):
                        content = a.get_text().strip()
                        tweets.append({
                            'id': i + 1,
                            'content': content,
                            'link': a.get('href', ''),
                            'username': 'Elon Musk',
                            'handle': '@elonmusk',
                            'source': name
                        })
                    break
                    
        except Exception as e:
            print(f"âœ— å¤±è´¥: {str(e)[:50]}")
            continue
    
    return tweets

def check_updates():
    """æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡"""
    global LAST_TWEET_LINK
    
    print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ£€æŸ¥æ¨æ–‡...")
    
    tweets = get_tweets_from_scrape()
    
    if not tweets:
        print("æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨ç©ºæ•°æ®")
    
    # æ£€æŸ¥æ–°æ¨æ–‡
    new_tweets = []
    if tweets and LAST_TWEET_LINK:
        for tweet in tweets:
            if tweet['link'] != LAST_TWEET_LINK:
                new_tweets.append(tweet)
    elif tweets:
        new_tweets = [tweets[0]]
    
    if new_tweets:
        print(f"ğŸ‰ æ£€æµ‹åˆ° {len(new_tweets)} æ¡æ–°æ¨æ–‡!")
        for t in new_tweets:
            print(f"  - {t['content'][:60]}...")
        if tweets:
            LAST_TWEET_LINK = tweets[0]['link']
    else:
        print("æœªæ£€æµ‹åˆ°æ–°æ¨æ–‡")
    
    # ä¿å­˜æ•°æ®
    data = {
        'last_updated': datetime.now().isoformat(),
        'count': len(tweets),
        'source': tweets[0]['source'] if tweets else 'none',
        'new_tweets': len(new_tweets),
        'tweets': tweets
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"å·²ä¿å­˜: {len(tweets)} æ¡æ¨æ–‡")

def main():
    """ä¸»å‡½æ•°"""
    global LAST_TWEET_LINK
    
    # è¯»å–ä¸Šæ¬¡ä¿å­˜çš„æ¨æ–‡ ID
    try:
        with open(OUTPUT_FILE, 'r') as f:
            old_data = json.load(f)
            if old_data.get('tweets'):
                LAST_TWEET_LINK = old_data['tweets'][0].get('link')
                print(f"ä»ç¼“å­˜æ¢å¤ï¼Œä¸Šæ¬¡æœ€æ–°: {str(LAST_TWEET_LINK)[:50]}...")
    except:
        pass
    
    check_updates()

if __name__ == "__main__":
    main()
