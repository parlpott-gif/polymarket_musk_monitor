#!/usr/bin/env python3
"""
é©¬æ–¯å…‹æ¨æ–‡ RSS ç›‘æ§è„šæœ¬
ä½¿ç”¨ Nitter/XCancel RSS è®¢é˜…è·å–æœ€æ–°æ¨æ–‡
"""

import feedparser
import json
import time
from datetime import datetime

# Config
OUTPUT_FILE = "/home/admin/polymarket_musk_monitor/tweets.json"
CHECK_INTERVAL = 300  # 5 minutes

# å¤šä¸ª RSS æºï¼Œä¾æ¬¡å°è¯•
RSS_SOURCES = [
    "https://xcancel.com/elonmusk/rss",
    "https://nitter.privacydev.net/elonmusk/rss",
    "https://nitter.poast.org/elonmusk/rss",
]

# è®°å½•ä¸Šæ¬¡æ¨æ–‡ ID
LAST_TWEET_LINK = None

def get_tweets_from_rss():
    """ä» RSS æºè·å–æ¨æ–‡"""
    tweets = []
    
    for rss_url in RSS_SOURCES:
        try:
            print(f"å°è¯•: {rss_url}")
            feed = feedparser.parse(rss_url, timeout=30)
            
            if feed.entries:
                print(f"âœ“ æˆåŠŸè·å– {len(feed.entries)} æ¡æ¨æ–‡")
                for entry in feed.entries[:10]:
                    # æå–æ¨æ–‡å†…å®¹
                    title = entry.get('title', '')
                    # æ¸…ç†æ ‡é¢˜ï¼Œå»æ‰ç”¨æˆ·åå‰ç¼€
                    if 'Elon Musk (@elonmusk): ' in title:
                        content = title.replace('Elon Musk (@elonmusk): ', '')
                    else:
                        content = title
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯å›å¤
                    is_reply = title.startswith('R to @')
                    
                    tweets.append({
                        'id': len(tweets) + 1,
                        'content': content.strip(),
                        'link': entry.get('link', ''),
                        'published': entry.get('published', ''),
                        'username': 'Elon Musk',
                        'handle': '@elonmusk',
                        'is_reply': is_reply,
                        'source': rss_url
                    })
                return tweets, rss_url
            else:
                print(f"âœ— æ— æ•°æ®: {rss_url}")
        except Exception as e:
            print(f"âœ— å¤±è´¥: {rss_url} - {e}")
            continue
    
    return None, None

def check_updates():
    """æ£€æŸ¥æ˜¯å¦æœ‰æ–°æ¨æ–‡"""
    global LAST_TWEET_LINK
    
    print(f"\n[{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}] æ£€æŸ¥æ¨æ–‡...")
    
    tweets, source = get_tweets_from_rss()
    
    if tweets is None:
        print("æ‰€æœ‰ RSS æºéƒ½å¤±è´¥ï¼Œä¿å­˜ç©ºæ•°æ®")
        tweets = []
        source = "none"
    
    # è¿‡æ»¤å›å¤ï¼ˆå¦‚æœ polymarket ä¸è®¡å›å¤ï¼‰
    original_tweets = [t for t in tweets if not t['is_reply']]
    reply_count = len(tweets) - len(original_tweets)
    
    # æ£€æŸ¥æ–°æ¨æ–‡
    new_tweets = []
    if tweets and LAST_TWEET_LINK:
        for tweet in tweets:
            if tweet['link'] != LAST_TWEET_LINK:
                new_tweets.append(tweet)
    elif tweets:
        new_tweets = [tweets[0]]
    
    if new_tweets:
        print(f"ğŸ‰ æ£€æµ‹åˆ° {len(new_tweets)} æ¡æ–°æ¨æ–‡!")
        LAST_TWEET_LINK = tweets[0]['link']
    
    # ä¿å­˜æ•°æ®
    data = {
        'last_updated': datetime.now().isoformat(),
        'count': len(tweets),
        'original_count': len(original_tweets),
        'reply_count': reply_count,
        'source': source,
        'new_tweets': len(new_tweets),
        'tweets': tweets
    }
    
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    print(f"å·²ä¿å­˜: {len(tweets)} æ¡æ¨æ–‡ ({len(original_tweets)} æ¡åŸåˆ›, {reply_count} æ¡å›å¤)")
    return len(new_tweets)

def main():
    """ä¸»å¾ªç¯"""
    global LAST_TWEET_LINK
    
    # è¯»å–ä¸Šæ¬¡ä¿å­˜çš„æ¨æ–‡ ID
    try:
        with open(OUTPUT_FILE, 'r') as f:
            old_data = json.load(f)
            if old_data.get('tweets'):
                LAST_TWEET_LINK = old_data['tweets'][0].get('link')
                print(f"ä»ç¼“å­˜æ¢å¤ï¼Œä¸Šæ¬¡æœ€æ–°æ¨æ–‡: {LAST_TWEET_LINK[:50]}...")
    except:
        pass
    
    # ç«‹å³æ£€æŸ¥ä¸€æ¬¡
    check_updates()
    
    print(f"\nç­‰å¾… {CHECK_INTERVAL} ç§’åå†æ¬¡æ£€æŸ¥...")

if __name__ == "__main__":
    main()
